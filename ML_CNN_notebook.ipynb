{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "n4GpMXhcJedB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4seWKMw6lze2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#some code for checking gpu\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "tpz6DOpBJcWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import keras\n",
        "import time, warnings\n",
        "import tensorflow\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import keras_tuner as kt\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "from sklearn.model_selection import KFold, train_test_split, LeaveOneGroupOut, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Conv1D,\n",
        "    MaxPooling1D,\n",
        "    BatchNormalization,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    Conv2D,\n",
        "    MaxPool2D,\n",
        "    Activation,\n",
        "    GlobalAveragePooling2D,\n",
        ")\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications.densenet import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "gmrrULj3J-qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(clip):\n",
        "    normalized_clip = (clip - np.min(clip)) / (np.max(clip) - np.min(clip))\n",
        "    return normalized_clip\n",
        "\n",
        "def conv_array(root_folder):\n",
        "    metadata = pd.read_csv('/content/drive/MyDrive/UrbanSound8kv2/Data_extracted_2/processed_data.csv')\n",
        "    folds = {}\n",
        "    for fold in metadata['fold'].unique():\n",
        "        print(f\"processing fold{fold}\")\n",
        "        fold_path = os.path.join(root_folder, f\"fold{fold}\")\n",
        "        image_data = []\n",
        "        all_labels = []\n",
        "        shape = (76,33)\n",
        "        if not os.path.exists(fold_path):\n",
        "            print(f\"fold{fold} folder not found error!\")\n",
        "            continue  # Skip if the folder doesn't exist\n",
        "        filenames = metadata.loc[metadata['fold'] == fold, 'slice_file_name'].values\n",
        "        for filename in tqdm.tqdm(filenames):\n",
        "            png_filename = filename.replace('.wav', '.png')\n",
        "            image_path = os.path.join(fold_path, png_filename)\n",
        "            image = Image.open(image_path)\n",
        "            row_num = metadata[metadata['slice_file_name'] == filename].index\n",
        "            if not row_num.empty:\n",
        "                image_array = np.array(image)\n",
        "                if not image_array.shape == shape:\n",
        "                    print(\"array has not expected shape error!\")\n",
        "                    continue\n",
        "                image_array = normalize(image_array)\n",
        "                image_array = image_array.reshape(76,33,1)\n",
        "                label = metadata.iloc[row_num]['labelID'].values[0]\n",
        "                all_labels.append(label)\n",
        "                image_data.append(image_array)\n",
        "        image_data = np.array(image_data)\n",
        "        all_labels = np.array(all_labels)\n",
        "        all_labels = to_categorical(all_labels, num_classes=10)\n",
        "        folds[f\"fold{fold}\"] = [image_data, all_labels]\n",
        "    return folds\n",
        "\n",
        "\n",
        "metadata = pd.read_csv('/content/drive/MyDrive/UrbanSound8kv2/Data_extracted_2/processed_data.csv')\n",
        "root_folder = r\"/content/drive/MyDrive/UrbanSound8kv2/Data_extracted_2/both\"\n",
        "data = conv_array(root_folder)\n",
        "input_shape = data['fold1'][0].shape\n",
        "print(input_shape)"
      ],
      "metadata": {
        "id": "m9ZWUJw6QP8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPwW-JU1JBFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11aba445-de15-4a1b-90a5-5754a342ed03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training using fold5 as validation\n",
            "Epoch 1/20\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tensorflow.keras.backend.clear_session()\n",
        "\n",
        "#Building a hypermodel:\n",
        "# function to build a hypermodel\n",
        "# takes an argument from which to sample hyperparameters\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    inputshape = data['fold1'][0]\n",
        "\n",
        "    model.add(Conv2D(hp.Int('input_units', min_value=32, max_value=256, step=32), (3, 3), input_shape=inputshape[1:]))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    for i in range(hp.Int('n_layers', 1, 4)):  # adding variation of layers, this parameter will have a convnet with 2â€“5 convolutions\n",
        "        model.add(Conv2D(hp.Int(f'conv_{i}_units', min_value=32, max_value=256, step=32), (3, 3)))\n",
        "        model.add(Activation('relu'))\n",
        "        # adding dropout\n",
        "        model.add(tensorflow.keras.layers.Dropout(rate=hp.Float('rate', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    for i in range(hp.Int('n_connections', 1, 4)):\n",
        "        model.add(Dense(hp.Choice(f'n_nodes',\n",
        "                                  values=[128, 256, 512, 1024])))\n",
        "        model.add(Activation('tanh'))\n",
        "\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3), #optimization algorithm used is Adam\n",
        "                  loss=loss,\n",
        "                  metrics=[metric])\n",
        "\n",
        "    return model\n",
        "\n",
        "#get optimal hyperparameters using\n",
        "def tuner(num_epoch, batch_size):\n",
        "  fold_name = \"fold1\"\n",
        "  print(f\"Training using {fold_name} as validation\")\n",
        "  X_val, y_val = data[fold_name][0], data[fold_name][1]\n",
        "  X_train = []\n",
        "  y_train = []\n",
        "\n",
        "  for other_fold_name, other_fold_data in data.items():\n",
        "    if other_fold_name == fold_name:\n",
        "        continue\n",
        "\n",
        "    X = other_fold_data[0]\n",
        "    y = other_fold_data[1]\n",
        "    X_train.extend(X)\n",
        "    y_train.extend(y)\n",
        "\n",
        "  X_train = np.array(X_train)\n",
        "  y_train = np.array(y_train)\n",
        "  EarlyStoppingCallback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stop)\n",
        "  tuner = RandomSearch(build_model, objective=objective, max_trials=max_trials, executions_per_trial=max_trial_retrys, metrics=[metric])\n",
        "  tuner.search(x=X_train, y=y_train, epochs=num_epoch, batch_size=batch_size, validation_data=(X_val, y_val), callbacks=[EarlyStoppingCallback]) #10% is validation data\n",
        "  best_hyperparameters = tuner.oracle.get_best_trials(1)[0].hyperparameters.values\n",
        "  return best_hyperparameters\n",
        "\n",
        "  #hyperparameters2 = tuner(X, y, num_epoch, batch_size)\n",
        "  #print(hyperparameters2)\n",
        "def model_k_cross(hyperparameters, data):\n",
        "    hp = kt.HyperParameters()\n",
        "    list_scores = []\n",
        "    for key, value in hyperparameters.items():\n",
        "        hp.Fixed(key, value)\n",
        "\n",
        "    for fold_name, fold_data in data.items():\n",
        "        print(f\"Training using {fold_name} as validation\")\n",
        "        X_val, y_val = fold_data[0], fold_data[1]\n",
        "        X_train = []\n",
        "        y_train = []\n",
        "\n",
        "        for other_fold_name, other_fold_data in data.items():\n",
        "            if other_fold_name == fold_name:\n",
        "                continue\n",
        "\n",
        "            X = other_fold_data[0]\n",
        "            y = other_fold_data[1]\n",
        "            X_train.extend(X)\n",
        "            y_train.extend(y)\n",
        "\n",
        "        X_train = np.array(X_train)\n",
        "        y_train = np.array(y_train)\n",
        "\n",
        "        cmodel = build_model(hp)\n",
        "        cmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        #callbacks:\n",
        "        #If the model sees no change in validation loss the ReduceLROnPlateau function will reduce the learning rate, which often benefits the model.\n",
        "        anne = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1, min_lr=1e-3)\n",
        "        EarlyStoppingCallback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stop)\n",
        "\n",
        "        history = cmodel.fit(X_train, y_train, epochs=num_epoch, batch_size=batch_size,\n",
        "                   callbacks=[anne, EarlyStoppingCallback], validation_data=(X_val, y_val))\n",
        "\n",
        "        # Evaluation\n",
        "        scores = cmodel.evaluate(X_val, y_val)\n",
        "        print(\"Validation accuracy:\", scores[1])\n",
        "        list_scores.append(scores[1])\n",
        "\n",
        "\n",
        "             # Plot training history - loss\n",
        "        print(history.history.keys())\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title(f\"Training Loss - {fold_name} as validation\")\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Training Loss', 'Validation Loss'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy'])\n",
        "        plt.title(f\"Accuracy - {fold_name} as validation\")\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    average_acc = sum(list_scores) / len(list_scores)\n",
        "    print(f'List of scores{list_scores}')\n",
        "    print(f'Average accuracy: {average_acc}')\n",
        "\n",
        "    total = 0\n",
        "    accurate = 0\n",
        "    accurateindex = []\n",
        "    wrongindex = []\n",
        "    label = ['air_conditioning', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music']\n",
        "\n",
        "    Ypred = cmodel.predict(X_val)\n",
        "    Ypred = np.argmax(Ypred, axis=1)\n",
        "    Ytrue = np.argmax(y_val, axis=1)\n",
        "    cm = confusion_matrix(Ytrue, Ypred)\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    ax = sns.heatmap(cm, cmap=\"rocket_r\", fmt=\".01f\",annot_kws={'size':16}, annot=True, square=True, xticklabels=label, yticklabels=label)\n",
        "    ax.set_ylabel('Actual', fontsize=20)\n",
        "    ax.set_xlabel('Predicted', fontsize=20)\n",
        "\n",
        "    for i in range(len(Ypred)):\n",
        "        if np.argmax(Ypred[i]) == np.argmax(y_val[i]):\n",
        "            accurate += 1\n",
        "            accurateindex.append(i)\n",
        "        else:\n",
        "            wrongindex.append(i)\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    print('Total-test-data;', total, '\\taccurately-predicted-data:', accurate, '\\t wrongly-predicted-data: ', total - accurate)\n",
        "    print('Accuracy:', round(accurate/total*100, 3), '%')\n",
        "\n",
        "\n",
        "\n",
        "#creating custom hyperparameters to inspect model performance,inspired by the network we found on kaggle\n",
        "custom_hyperparameters = {\n",
        "        'input_units': 224,\n",
        "        'n_layers': 4,\n",
        "        'conv_0_units': 64,\n",
        "        'rate': 0.2,\n",
        "        'n_connections': 3,\n",
        "        'n_nodes': 1012,\n",
        "        'conv_1_units': 128,\n",
        "    }\n",
        "\n",
        "\n",
        "metric = 'accuracy' #evaluation metric\n",
        "loss= 'categorical_crossentropy' #loss function\n",
        "\n",
        "#training parameters\n",
        "num_epoch = 20\n",
        "batch_size = 256\n",
        "early_stop = 5 # early stoppping after 6 epochs with no improvement of test data\n",
        "\n",
        "#objective to specify the objective to select the best models, and we use max_trials to specify the number of different models to try.\n",
        "objective='val_loss'\n",
        "max_trials = 8 # how many model variations to test?\n",
        "max_trial_retrys = 3 # how many trials per variation? (same model could perform differently)\n",
        "\n",
        "\n",
        "model_k_cross(custom_hyperparameters, data)\n",
        "best_hyperparameters_overall = tuner(num_epoch,batch_size)\n",
        "model_k_cross(best_hyperparameters_overall, data)\n",
        "\n",
        "\n"
      ]
    }
  ]
}